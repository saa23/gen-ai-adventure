{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import IPython\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import comet_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate final results from model after calling OpenAI API\n",
    "def get_completion(messages, model=\"gpt-3.5-turbo\", temperature=0, max_tokens=300):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "def print_markdown(text):\n",
    "    \"\"\"Prints  text as markdown\"\"\"\n",
    "    IPython.display.display(IPython.display.Markdown(text))\n",
    "\n",
    "\n",
    "# load validation data from GitHub\n",
    "f = urllib.request.urlopen(\"https://raw.githubusercontent.com/comet-ml/comet-llmops/main/data/article-tags.json\")\n",
    "val_data = json.load(f)\n",
    "\n",
    "\n",
    "# load few shot data from GitHub\n",
    "f = urllib.request.urlopen(\"https://raw.githubusercontent.com/comet-ml/comet-llmops/main/data/few_shot.json\")\n",
    "few_shot_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to obtain final predictions from the model given a prompt template (e.g. zero-shot or few-shot) and the provided input data\n",
    "\n",
    "def get_predictions(prompt_template, inputs):\n",
    "    responses = []\n",
    "\n",
    "    for i in range(len(inputs)):\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": prompt_template.format(input=inputs[i])\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        response = get_completion(messages)\n",
    "        responses.append(response)\n",
    "        \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to define the few-shot template\n",
    "def get_few_shot_template(few_shot_prefix, few_shot_suffix, few_shot_examples):\n",
    "    return few_shot_prefix + \"\\n\" +\"\\n\".join([ \"Abstract: \"+ ex[\"abstract\"] + \"\\n\" + \"Tags: \" + str(ex[\"tags\"]) + \"\\n\" for ex in few_shot_examples]) + \"\\n\\n\" + few_shot_suffix\n",
    "\n",
    "# function to sample few shot data\n",
    "def random_sample_data (data, n):\n",
    "    return np.random.choice(few_shot_data, n, replace=False)\n",
    "\n",
    "\n",
    "# the few-shot prefix and suffix\n",
    "few_shot_prefix = \"\"\"Your task is to extract model names from machine learning paper abstracts. Your response is an an array of the model names in the format [\\\"model_name\\\"]. If you don't find model names in the abstract or you are not sure, return [\\\"NA\\\"]\n",
    "Use the following examples to help with steering your responses:\n",
    "\"\"\"\n",
    "few_shot_suffix = \"\"\"Abstract: {input}\\nTags:\"\"\"\n",
    "\n",
    "# load 3 samples from few shot data\n",
    "few_shot_template = get_few_shot_template(few_shot_prefix, few_shot_suffix, random_sample_data(few_shot_data, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your task is to extract model names from machine learning paper abstracts. Your response is an an array of the model names in the format [\"model_name\"]. If you don\\'t find model names in the abstract or you are not sure, return [\"NA\"]\\nUse the following examples to help with steering your responses:\\nAbstract: Children\\'s drawings have a wonderful inventiveness, creativity, and variety to them. We present a system that automatically animates children\\'s drawings of the human figure, is robust to the variance inherent in these depictions, and is simple and straightforward enough for anyone to use. We demonstrate the value and broad appeal of our approach by building and releasing the Animated Drawings Demo, a freely available public website that has been used by millions of people around the world. We present a set of experiments exploring the amount of training data needed for fine-tuning, as well as a perceptual study demonstrating the appeal of a novel twisted perspective retargeting technique. Finally, we introduce the Amateur Drawings Dataset, a first-of-its-kind annotated dataset, collected via the public demo, containing over 178,000 amateur drawings and corresponding user-accepted character bounding boxes, segmentation masks, and joint location annotations.\\nTags: [\\'NA\\']\\n\\nAbstract: Prevalent semantic segmentation solutions are, in essence, a dense discriminative classifier of p(class|pixel feature). Though straightforward, this de facto paradigm neglects the underlying data distribution p(pixel feature|class), and struggles to identify out-of-distribution data. Going beyond this, we propose GMMSeg, a new family of segmentation models that rely on a dense generative classifier for the joint distribution p(pixel feature,class). For each class, GMMSeg builds Gaussian Mixture Models (GMMs) via Expectation-Maximization (EM), so as to capture class-conditional densities. Meanwhile, the deep dense representation is end-to-end trained in a discriminative manner, i.e., maximizing p(class|pixel feature). This endows GMMSeg with the strengths of both generative and discriminative models. With a variety of segmentation architectures and backbones, GMMSeg outperforms the discriminative counterparts on three closed-set datasets. More impressively, without any modification, GMMSeg even performs well on open-world datasets. We believe this work brings fundamental insights into the related fields.\\nTags: [\\'GMMSeg\\', \\'GMMs\\']\\n\\nAbstract: The rapid advancement of conversational and chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents and provide insight into their \"cognitive\" processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of chat agents, providing a valuable resource for investigating conversational language models. Our contributions include introducing a novel communicative agent framework, offering a scalable approach for studying the cooperative behaviors and capabilities of multi-agent systems, and open-sourcing our library to support research on communicative agents and beyond. The GitHub repository of this project is made publicly available on:\\nTags: [\\'NA\\']\\n\\n\\nAbstract: {input}\\nTags:'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_template = \"\"\"\n",
    "Your task is extract model names from machine learning paper abstracts. Your response is an an array of the model names in the format [\\\"model_name\\\"]. If you don't find model names in the abstract or you are not sure, return [\\\"NA\\\"]\n",
    "\n",
    "Abstract: {input}\n",
    "Tags:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the validation data as inputs\n",
    "\n",
    "abstracts = [val_data[i][\"abstract\"] for i in range(len(val_data))]\n",
    "few_shot_predictions = get_predictions(few_shot_template, abstracts)\n",
    "zero_shot_predictions = get_predictions(zero_shot_template, abstracts)\n",
    "expected_tags = [str(val_data[i][\"tags\"]) for i in range(len(val_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few shot predictions\n",
      "[\"['Evol-Instruct', 'LLaMA', 'WizardLM', 'ChatGPT']\", \"['FLAN-T5', 'AMR', 'AMR2.0', 'AMR3.0', 'BioAMR', 'LoRA']\", \"['NA']\", \"['PAXQA', 'QG']\", \"['ChatGPT']\", \"['ViT', 'OpenCLIP']\", \"['SAM', 'IA', 'AIGC', 'Stable Diffusion']\", \"['Anything-3D', 'BLIP', 'Segment-Anything', 'text-to-image diffusion model']\", \"['Chameleon', 'GPT-4', 'ChatGPT']\", \"['NA']\"]\n",
      "\n",
      "Zero shot predictions\n",
      "['[\"Evol-Instruct\", \"LLaMA\", \"WizardLM\", \"OpenAI ChatGPT\"]', '[\"FLAN-T5\"]', '[\"large language models\", \"generative AI\", \"hypothesis machines\"]', '[\"PAXQA\"]', '[\"ChatGPT\"]', '[\"ViT\", \"OpenCLIP\"]', '[\"Segment-Anything Model (SAM)\", \"Inpaint Anything (IA)\", \"AIGC models\", \"Stable Diffusion\", \"Inpaint Anything (IA)\"]', '[\"Anything-3D\", \"BLIP\", \"Segment-Anything\", \"text-to-image diffusion model\", \"neural radiance field\"]', '[\"Chameleon\", \"GPT-4\", \"ScienceQA\", \"TabMWP\", \"ChatGPT\"]', '[\"NA\"]']\n",
      "\n",
      "Expected tags\n",
      "[\"['LLaMA', 'ChatGPT', 'WizardLM']\", \"['FLAN-T5', 'FLAN']\", \"['NA']\", \"['PAXQA']\", \"['ChatGPT']\", \"['OpenCLIP', 'ViT']\", \"['SAM', 'IA']\", \"['Anything-3D', 'BLIP', 'Segment-Anything']\", \"['Chameleon', 'GPT-4', 'ChatGPT']\", \"['NA']\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"Few shot predictions\")\n",
    "print(few_shot_predictions)\n",
    "print(\"\\nZero shot predictions\")\n",
    "print(zero_shot_predictions)\n",
    "print(\"\\nExpected tags\")\n",
    "print(expected_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Prompt Results\n",
    "Log prompt & results to Comet.\n",
    "\n",
    "Track both prompt few-shot and zero-shot with the metadata and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Valid Comet API Key saved in C:\\Users\\achma\\.comet.config (set COMET_CONFIG to change where it is saved).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt logged to https://www.comet.com/llmops-exercise/ml-paper-tagger-prompts\n"
     ]
    }
   ],
   "source": [
    "COMET_API_KEY = os.getenv(\"COMET_API_KEY\")\n",
    "COMET_WORKSPACE = os.getenv(\"COMET_WORKSPACE\")\n",
    "\n",
    "\n",
    "# initialize comet\n",
    "comet_llm.init(COMET_API_KEY, COMET_WORKSPACE, project=\"ml-paper-tagger-prompts\")\n",
    "\n",
    "# log the predictions\n",
    "for i in range(len(expected_tags)):\n",
    "    # log the few-shot predictions\n",
    "    comet_llm.log_prompt(\n",
    "        prompt=few_shot_template.format(input=abstracts[i]),\n",
    "        prompt_template=few_shot_template,\n",
    "        output=few_shot_predictions[i],\n",
    "        tags = [\"gpt-3.5-turbo\", \"few-shot\"],\n",
    "        metadata = {\n",
    "            \"expected_tags\": expected_tags[i],\n",
    "            \"abstract\": abstracts[i],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # log the zero-shot predictions\n",
    "    comet_llm.log_prompt(\n",
    "        prompt=zero_shot_template.format(input=abstracts[i]),\n",
    "        prompt_template=zero_shot_template,\n",
    "        output=zero_shot_predictions[i],\n",
    "        tags = [\"gpt-3.5-turbo\", \"zero-shot\"],\n",
    "        metadata = {\n",
    "            \"expected_tags\": expected_tags[i],\n",
    "            \"abstract\": abstracts[i],\n",
    "        }\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
